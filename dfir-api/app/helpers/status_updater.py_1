import json
from datetime import datetime
from app.db.database import async_session_maker
from app.db_helper.action_job_details import insert_action_log
import logging

STATUS_KEY_PREFIX = "snapshot_status:"

async def fail_and_cleanup(redis, job_id: str):
    """Remove job status from redis and perform any additional cleanup."""
    try:
        await redis.delete(f"{STATUS_KEY_PREFIX}{job_id}")
        # If you have a background task manager, add code here to remove/cancel the job
        # Example: await arq_queue.cancel(job_id)  # Uncomment and adapt if using ARQ
        logging.info(f"[fail_and_cleanup] Cleared job {job_id} from redis and background tasks.")
    except Exception as e:
        logging.error(f"[fail_and_cleanup] Failed to cleanup job {job_id}: {e}")

async def set_status(redis, job_id: str, account_id: str, case_number: str, stage: str, data: dict = None):
    payload = {
        "stage": stage,
        "timestamp": datetime.utcnow().isoformat(),
        "account_id": account_id,
        "case_number": case_number,
        "data": data or {}
    }
    await redis.set(f"{STATUS_KEY_PREFIX}{job_id}", json.dumps(payload))

    async with async_session_maker() as session:
        instance_id = data.get("instance_id") if data else None
        volume_id = data.get("volume_id") if data else None
        snapshot_id = data.get("snapshot_id") if data else None
        progress = int(data.get("progress", "0").replace("%", "")) if data and "progress" in data else None
        message = data.get("error") or None
        percentage = data.get("percentage") or progress
        errors = data.get("errors")
        try:
            await insert_action_log(
                session=session,
                job_id=job_id,
                instance_id=instance_id,
                action=stage,
                status="in_progress" if stage != "completed" and stage != "failed" else stage,
                volume_id=volume_id,
                snapshot_id=snapshot_id,
                progress=progress,
                message=message,
                details=data,
                case_number=case_number,
                account_id=account_id,
                percentage=percentage,
                errors=errors
            )
            logging.info(f"[set_status] Successfully inserted action log for job_id={job_id}, stage={stage}, case_number={case_number}")
        except Exception as e:
            logging.error(f"[set_status] Failed to insert action log for job_id={job_id}, stage={stage}, case_number={case_number}: {e}")
            logging.error(f"[set_status] Payload: job_id={job_id}, instance_id={instance_id}, action={stage}, status={'in_progress' if stage != 'completed' and stage != 'failed' else stage}, volume_id={volume_id}, snapshot_id={snapshot_id}, progress={progress}, message={message}, details={data}, case_number={case_number}, account_id={account_id}, percentage={percentage}, errors={errors}")
    # If this is a failure state, cleanup redis and background task
    if stage == "failed" or (data and (data.get("error") or data.get("errors"))):
        await fail_and_cleanup(redis, job_id)

async def periodic_status_updater(redis, job_id, account_id, case_number, stage, data_fn, interval=20, stop_condition_fn=None):
    """
    Call set_status every `interval` seconds with data from data_fn().
    Optionally stop if stop_condition_fn() returns True.
    """
    import asyncio
    while True:
        data = await data_fn()
        await set_status(redis, job_id, account_id, case_number, stage, data)
        if stop_condition_fn and await stop_condition_fn():
            break
        await asyncio.sleep(interval)

# Usage example for a snapshot progress loop:
# async def get_snapshot_status():
#     ... # fetch latest status from AWS
#     return {"instance_id": ..., "progress": ..., ...}
# await periodic_status_updater(redis, job_id, account_id, case_number, "snapshot_in_progress", get_snapshot_status)


